{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the numpy and the matplotlib libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000025       5       1 ...       1       1       2]\n",
      " [1002945       5       4 ...       2       1       2]\n",
      " [1015425       3       1 ...       1       1       2]\n",
      " ...\n",
      " [ 888820       5      10 ...      10       2       4]\n",
      " [ 897471       4       8 ...       6       1       4]\n",
      " [ 897471       4       8 ...       4       1       4]]\n"
     ]
    }
   ],
   "source": [
    "# read the CSV file into the two-dimensional array \"data\"\n",
    "# using numpy's function genfromtxt\n",
    "data_set=np.genfromtxt('breast-cancer-wisconsin.data.csv',delimiter=',')\n",
    "data_set = data_set.astype('int64')\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the array \"data\"\n",
    "\n",
    "print(\"Shape of data set array\", data_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 rows of the array \"data\"\n",
    "print(\"Printing first five rows of data set\")\n",
    "data_set[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to the array \"X\" the features from the array \"data\"\n",
    "# do we need to ignore any feature (column)?\n",
    "\n",
    "# Removing the unique patient id from features which is doesn't have any relation with output i.e data preprocessing \n",
    "X=np.copy(data_set[:,1:10]) \n",
    "print(\"Dataset after removing patient id\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to the array \"y\" the labels from the array \"data\"\n",
    "# Do we need to adjust the values in \"y\"?\n",
    "\n",
    "#Converting the actual output 2,4 to 0's and 1's\n",
    "\n",
    "Y=np.copy(data_set[:,10:]) # Storing Expected Output in Y\n",
    "y_conversion_list=[]\n",
    "for i in Y:\n",
    "    i=((i-2)/2)\n",
    "    y_conversion_list.append(i)\n",
    "Y=np.array(y_conversion_list)  # Actual output\n",
    "y=np.array(Y)\n",
    "y=y.astype(int)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X (the features) and y (the labels) into traing and testing sets:\n",
    "# X_train, y_train, X_test, y_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=47)\n",
    "y_train=y_train.T \n",
    "y_test=y_test.T\n",
    "#Printing the splitted data for testing and training the model\n",
    "print(\"X_train data is: \",X_train)\n",
    "print(\"y_train data is: \",y_train)\n",
    "print(\"X_test data is: \",X_test)\n",
    "print(\"y_test data is: \",y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm we discussed in class assumes that the features array to have the dimensions n rows by m columns\n",
    "# where n is the number of features and m is the size of the data\n",
    "# Make sure that X_train and X_test have the correct dimensions\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid function and the derivative of the sigmoid function\n",
    "\n",
    "#Activation function sigmoid is defined below\\\n",
    "\n",
    "def g1(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "def g2(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "def g1_prime(x):\n",
    "    return g1(x) * (1 - g1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to initialize the weight and bias matrices\n",
    "\n",
    "def intialize(n_h,n_x,n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "    b1 = np.random.randn(n_h, 1)\n",
    "    W2 = np.random.randn(n_y, n_h)\n",
    "    b2 = np.random.randn(n_y, 1)\n",
    "    \n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the function \"nn\" that does the training of a model\n",
    "def NN(X, y, n_h, n_y, alpha, iterations):\n",
    "    stage_1 = iterations//2; alpha2 = alpha/2.\n",
    "    stage_2 = iterations//1.75; alpha3 = alpha/4.\n",
    "    stage_3 = iterations//1.5; alpha4 = alpha/8.\n",
    "    cost_list = [[],[]]\n",
    "    m = X.shape[1]\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    #Calling the above declared intialize method to set the weight and bias matrices \n",
    "    W1,b1,W2,b2=intialize(n_h,n_x,n_y)\n",
    "   \n",
    "    for i in range(iterations):\n",
    "        # Forward propagation \n",
    "        Z1 = np.dot(W1, X) + b1\n",
    "        A1 = g1(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = g2(Z2)\n",
    "        \n",
    "        #Loss function\n",
    "        if i%100 == 0:\n",
    "            # Usage of Cross Entropy to calculate loss\n",
    "            cost = -np.sum(y*np.log(A2) + (1-y)*np.log(1-A2))\n",
    "            # Using root mean square method to calculate loss\n",
    "            #cost = np.sum(np.sqrt(((y-A2)**2).mean()))\n",
    "            cost_list[0].append(i)\n",
    "            cost_list[1].append(cost)\n",
    "        #Backward propagation  \n",
    "        dZ2 = A2 - y\n",
    "        dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2)\n",
    "        dZ1 = np.dot(W2.T, dZ2) * g1_prime(Z1)\n",
    "        dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1)\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "        W1 = W1 - alpha * dW1\n",
    "        b1 = b1 - alpha * db1\n",
    "        if i>stage_1: alpha = alpha2\n",
    "        elif i>stage_2: alpha = alpha3\n",
    "        elif i>stage_3: alpha = alpha4\n",
    "    return W1, b1, W2, b2, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters needed for traing the model\n",
    "# Initialize your weight matrices\n",
    "# call the the \"nn\" function to train the model on the testing data\n",
    "\n",
    "#Intializing the input data to train the model\n",
    "X = np.array(X_train).T \n",
    "#Intializing the expected output data to feed the model\n",
    "y = np.array(y_train)\n",
    "n_h = 8; n_y = 1\n",
    "\n",
    "#Alpha is the learning rate \n",
    "alpha = 0.05\n",
    "iterations = 50000\n",
    "\n",
    "#Calling the NN function to train the model by providing the required parameters \n",
    "W1, b1, W2, b2, cost_list = NN(X, y, n_h, n_y, alpha, iterations)\n",
    "\n",
    "#Printing the final adjusted parameters to train the model\n",
    "print('W1=',W1, '\\n', 'b1=',b1, '\\n', 'W2=',W2, '\\n', 'b2=',b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a plot of the loss as a function of the number of iterations\n",
    "\n",
    "plt.plot(cost_list[0][1:], cost_list[1][1:], 'go')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the trained model, predict the labels for the testing data\n",
    "\n",
    "#Testing the model with the test data \n",
    "\n",
    "Z1 = np.dot(W1, X_test.T) + b1\n",
    "A1 = g1(Z1)\n",
    "Z2 = np.dot(W2, A1) + b2\n",
    "A2 = g2(Z2)\n",
    "predictions = A2.copy()\n",
    "predictions[A2 < 0.5] = 0\n",
    "predictions[A2 > 0.5] = 1\n",
    "print('A2=',A2)\n",
    "print('predictions=',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the array of the predicted labels has the same dimension as the testing data\n",
    "\n",
    "print(\"Shape of testing data:\", y_test.shape)\n",
    "print(\"Shape of predictions data: \",predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confusion matrix for the testing data\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "print(confusion_matrix(y_test[0], predictions[0]))  \n",
    "print(classification_report(y_test[0], predictions[0])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "    \n",
    "Below is the confusion matrix and classification report when the model is trained with 80 perecent training data and tested with 20 percent testing data and the number of neurons are 8 and iterations are 50000 here.\n",
    "\n",
    "[[84  1]\n",
    " [ 3 52]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.99      0.98        85\n",
    "           1       0.98      0.95      0.96        55\n",
    "\n",
    "    accuracy                           0.97       140\n",
    "   macro avg       0.97      0.97      0.97       140\n",
    "weighted avg       0.97      0.97      0.97       140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "\n",
    "Below is the confusion matrix and classification report when the model is trained the 5 neurons in the hidden layer\n",
    "[[84  1]\n",
    " [ 4 51]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.99      0.97        85\n",
    "           1       0.98      0.93      0.95        55\n",
    "\n",
    "    accuracy                           0.96       140\n",
    "   macro avg       0.97      0.96      0.96       140\n",
    "weighted avg       0.96      0.96      0.96       140\n",
    "\n",
    "\n",
    "Below is the confusion matrix and classification report when the model is trained the 10 neurons in the hidden layer\n",
    "\n",
    "[[84  1]\n",
    " [ 4 51]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.99      0.97        85\n",
    "           1       0.98      0.93      0.95        55\n",
    "\n",
    "    accuracy                           0.96       140\n",
    "   macro avg       0.97      0.96      0.96       140\n",
    "weighted avg       0.96      0.96      0.96       140\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5:\n",
    "\n",
    "Below is the confusion matrix and classification report when the model is trained with th 90 perecent training data and tested with 10 percent testing data\n",
    "\n",
    "[[44  1]\n",
    " [ 2 23]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.98      0.97        45\n",
    "           1       0.96      0.92      0.94        25\n",
    "\n",
    "    accuracy                           0.96        70\n",
    "   macro avg       0.96      0.95      0.95        70\n",
    "weighted avg       0.96      0.96      0.96        70\n",
    "\n",
    "Below is the confusion matrix and classification report when the model is trained with th 60 perecent training data and tested with 40 percent testing data\n",
    "\n",
    "[[171  10]\n",
    " [  7  92]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.94      0.95       181\n",
    "           1       0.90      0.93      0.92        99\n",
    "\n",
    "    accuracy                           0.94       280\n",
    "   macro avg       0.93      0.94      0.93       280\n",
    "weighted avg       0.94      0.94      0.94       280\n",
    "\n",
    "\n",
    "Conclusion: Accuracy is a bit high when the model is feed with 90 percent of the data for training the model compared to the 60 percent \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6:\n",
    "\n",
    "Below is the confusion matrix, classification report and acurracy of model when we use the cross entropy as the loss function  \n",
    "\n",
    "\n",
    "[[84  1]\n",
    " [ 3 52]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.99      0.98        85\n",
    "           1       0.98      0.95      0.96        55\n",
    "\n",
    "    accuracy                           0.97       140\n",
    "   macro avg       0.97      0.97      0.97       140\n",
    "weighted avg       0.97      0.97      0.97       140\n",
    "\n",
    "\n",
    "Below is the confusion matrix, classification report and acurracy of model when we use the Root Mean square as the loss function \n",
    "\n",
    "[[84  1]\n",
    " [ 5 50]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.99      0.97        85\n",
    "           1       0.98      0.91      0.94        55\n",
    "\n",
    "    accuracy                           0.96       140\n",
    "   macro avg       0.96      0.95      0.95       140\n",
    "weighted avg       0.96      0.96      0.96       140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using batches to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_batchModel(X, y, n_h, n_y, alpha, iterations):\n",
    "    stage_1 = iterations//2; alpha2 = alpha/2.\n",
    "    stage_2 = iterations//1.75; alpha3 = alpha/4.\n",
    "    stage_3 = iterations//1.5; alpha4 = alpha/8.\n",
    "    cost_list = [[],[]]\n",
    "    m = X.shape[1]\n",
    "    n_x = X.shape[0]\n",
    "    batch_size=64\n",
    "    W1,b1,W2,b2=intialize(n_h,n_x,n_y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        for j in range(0, m, batch_size):\n",
    "            X_batch = X[:, j:j+batch_size]\n",
    "            y_batch = y[j:j+batch_size]\n",
    "            Z1 = np.dot(W1, X_batch) + b1\n",
    "            A1 = g1(Z1)\n",
    "            Z2 = np.dot(W2, A1) + b2\n",
    "            A2 = g1(Z2)\n",
    "            dZ2 = A2 - y_batch\n",
    "            dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "            db2 = (1/m) * np.sum(dZ2)\n",
    "            dZ1 = np.dot(W2.T, dZ2) * g1_prime(Z1)\n",
    "            dW1 = (1/m) * np.dot(dZ1, X_batch.T)\n",
    "            db1 = (1/m) * np.sum(dZ1)\n",
    "            W2 = W2 - alpha * dW2\n",
    "            b2 = b2 - alpha * db2\n",
    "            W1 = W1 - alpha * dW1\n",
    "            b1 = b1 - alpha * db1\n",
    "            if i>stage_3: alpha = alpha4\n",
    "            elif i>stage_2: alpha = alpha3\n",
    "            elif i>stage_1: alpha = alpha2\n",
    "                \n",
    "        if i%2 == 0:\n",
    "            Z1 = np.dot(W1, X) + b1\n",
    "            A1 = g1(Z1)\n",
    "            Z2 = np.dot(W2, A1) + b2\n",
    "            A2 = g1(Z2)\n",
    "            cost = -np.sum(y*np.log(A2) + (1-y)*np.log(1-A2))\n",
    "            cost_list[0].append(i)\n",
    "            cost_list[1].append(cost)\n",
    "    return W1, b1, W2, b2, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X_train).T \n",
    "#Intializing the expected output data to feed the model\n",
    "y = np.ravel((y_train))\n",
    "\n",
    "n_h = 8; n_y = 1\n",
    "\n",
    "#Alpha is the learning rate \n",
    "alpha = 0.05\n",
    "iterations = 10000\n",
    "\n",
    "#Calling the NN function to train the model by providing the required parameters \n",
    "W1, b1, W2, b2, cost_list = NN_batchModel(X, y, n_h, n_y, alpha, iterations)\n",
    "\n",
    "#Printing the final adjusted parameters to train the model\n",
    "print('W1=',W1, '\\n', 'b1=',b1, '\\n', 'W2=',W2, '\\n', 'b2=',b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the loss as a function of the number of iterations when using batches to train the model\n",
    "\n",
    "plt.plot(cost_list[0][1:], cost_list[1][1:], 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the model after it being trained using the batches \n",
    "\n",
    "Z1 = np.dot(W1, X_test.T) + b1\n",
    "A1 = g1(Z1)\n",
    "Z2 = np.dot(W2, A1) + b2\n",
    "A2 = g2(Z2)\n",
    "predictions = A2.copy()\n",
    "predictions[A2 < 0.5] = 0\n",
    "predictions[A2 > 0.5] = 1\n",
    "print('predictions=',predictions)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "print(confusion_matrix(y_test[0], predictions[0]))  \n",
    "print(classification_report(y_test[0], predictions[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7\n",
    "\n",
    "Below is the confusion matrix and classification report when the batch size is used as 32 and number of iterations used here are less compared to the normal training model i.e 10000 iterations\n",
    "\n",
    "[[84  1]\n",
    " [ 3 52]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.99      0.98        85\n",
    "           1       0.98      0.95      0.96        55\n",
    "\n",
    "    accuracy                           0.97       140\n",
    "   macro avg       0.97      0.97      0.97       140\n",
    "weighted avg       0.97      0.97      0.97       140\n",
    "\n",
    "\n",
    "Below is the confusion matrix and classification report when the batch size is used as 64\n",
    "\n",
    "[[84  1]\n",
    " [ 4 51]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.99      0.97        85\n",
    "           1       0.98      0.93      0.95        55\n",
    "\n",
    "    accuracy                           0.96       140\n",
    "   macro avg       0.97      0.96      0.96       140\n",
    "weighted avg       0.96      0.96      0.96       140\n",
    "\n",
    "\n",
    "Conclusion: When the batch size is increased the training time is decreased but the accuracy of the model is reduced, so the smaller batch sizes gives more accuracy compared to the larger batch size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
